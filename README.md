<div align="center">
  <a href="https://swarms.world">
    <h1>LLM-in-the-loop Machine Learning</h1>
  </a>
</div>
<p align="center">
  <em>Position: Embracing LLM Application with LLM-in-the-loop Machine Learning</em>
</p>

----

This page provides a comprehensive reading list on LLM-in-the-loop machine learning. The empirical experiment conducted for LLM-native text clustering can be found in the `./experiment` directory.

---

## Table of Contents
- [Table of Contents](#table-of-contents)
- [LLM-in-the-Loop Keyword Paper](#llm-in-the-loop-keyword-paper)
- [Data-Centric LLM-in-the-loop](#data-centric-llm-in-the-loop)
  - [Data Annotation](#data-annotation)
  - [Augmentation](#augmentation)
  - [Feature Engineering](#feature-engineering)
- [Model-Centric LLM-in-the-loop](#model-centric-llm-in-the-loop)
  - [Active Learning With LLM](#active-learning-with-llm)
  - [Reinforcement Learning With LLM](#reinforcement-learning-with-llm)
- [Task-Centric LLM-in-the-loop](#task-centric-llm-in-the-loop)
- [Crowdsourcing](#crowdsourcing)
- [Summary](#summary)

---

## LLM-in-the-Loop Keyword Paper
1. Neural Topic Modeling with Large Language Models in the Loop [[paper]](https://arxiv.org/pdf/2411.08534)
2. LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning [[paper]](https://arxiv.org/pdf/2408.02999)
3. Asynchronous Large Language Model Enhanced Planner for Autonomous Driving [[paper]](https://arxiv.org/pdf/2406.14556)
4. Language Models in the Loop: Incorporating Prompting into Weak Supervision [[paper]](https://arxiv.org/pdf/2205.02318)
5. Dial-In LLM: Human-Aligned Dialogue Intent Clustering with LLM-in-the-loop [[paper]](https://arxiv.org/pdf/2412.09049)
6. LLM-in-the-loop: Leveraging Large Language Model for Thematic Analysis [[paper]](https://aclanthology.org/2023.findings-emnlp.669.pdf)
7. Uncovering Latent Arguments in Social Media Messaging by Employing LLMs-in-the-Loop Strategy [[paper]](https://arxiv.org/pdf/2404.10259)
8. LLMs in the Loop: Leveraging Large Language Model Annotations for Active Learning in Low-Resource Languages [[paper]](https://aclanthology.org/2023.findings-emnlp.669.pdf)
9. Generalized Category Discovery with Large Language Models in the Loop [[paper]](https://aclanthology.org/2024.findings-acl.512.pdf)
10. Generative AI-in-the-loop: Integrating LLMs and GPTs into the Next Generation Networks [[paper]](https://arxiv.org/pdf/2406.04276)
11. Hierarchical LLMs In-the-loop Optimization for Real-time Multi-Robot Target Tracking under Unknown Hazards [[paper]](https://arxiv.org/pdf/2409.12274)
12. Training LLMs to Recognize Hedges in Spontaneous Narratives [[paper]](https://arxiv.org/pdf/2408.03319)
13. LLMs-in-the-loop Part-1: Expert Small AI Models for Bio-Medical Text Translation [[paper]](https://arxiv.org/pdf/2407.12126)
14. A Rationale-centric Counterfactual Data Augmentation Method for Cross-Document Event Coreference Resolution [[paper]](https://arxiv.org/pdf/2404.01921)
15. Towards Single-System Illusion in Software-Defined Vehicles – Automated, AI-Powered Workflow [[paper]](https://arxiv.org/pdf/2403.14460)
16. Instances Need More Care: Rewriting Prompts for Instances with LLMs in the Loop Yields Better Zero-Shot Performance [[paper]](https://aclanthology.org/2024.findings-acl.371.pdf)

---

## Data-Centric LLM-in-the-loop

### Data Annotation
1. Large language models for data annotation and synthesis: A survey [[paper]](https://aclanthology.org/2024.emnlp-main.54.pdf)
2. Investigating and mitigating biases in crowdsourced data [[paper]](https://arxiv.org/pdf/2111.14322)
3. Modeling and mitigating human annotation errors to design efficient stream processing systems with human-in-theloop machine learning [[paper]](https://arxiv.org/pdf/2007.03177)
4. Chatgpt outperforms crowd workers for text-annotation tasks [[paper]](https://arxiv.org/pdf/2303.15056)
5. ChatGPT: Beginning of an End of Manual Linguistic Data Annotation? Use Case of Automatic Genre Identification [[paper]](https://arxiv.org/abs/2303.03953)
6. Chatgpt-4 outperforms experts and crowd workers in annotating political twitter messages with zeroshot learning [[paper]](https://arxiv.org/pdf/2304.06588)
7. An empirical study on challenges for llm application developers [[paper]](https://arxiv.org/pdf/2408.05002)
8.  AnnoLLM: Making large language models to be better crowdsourced annotators [[paper]](https://aclanthology.org/2024.naacl-industry.15.pdf)
9.  Multi-news+: Costefficient dataset cleansing via LLM-based data annotation [[paper]](https://aclanthology.org/2024.emnlp-main.2.pdf)
10.  Language models in the loop: Incorporating prompting into weak supervision [[paper]](https://arxiv.org/pdf/2205.02318)

### Augmentation
1. Data augmentation can improve robustness [[paper]](https://arxiv.org/pdf/2111.05328)
2. Expanding chatbot knowledge in customer service: Context-aware similar question generation using large language models [[paper]](https://arxiv.org/pdf/2410.12444)
3. A new benchmark and reverse validation method for passage-level hallucination detection [[paper]](https://aclanthology.org/2023.findings-emnlp.256.pdf)
4. Large language model as attributed training data generator: A tale of diversity and bias [[paper]](https://arxiv.org/pdf/2306.15895)
5. Fusegen: Plm fusion for data-generation based zero-shot learning [[paper]](https://arxiv.org/pdf/2406.12527)
6. Multi-news+: Costefficient dataset cleansing via LLM-based data annotation [[paper]](https://aclanthology.org/2024.emnlp-main.2.pdf)
7. Fill in the gaps: Model calibration and generalization with synthetic data [[paper]](https://aclanthology.org/2024.emnlp-main.955.pdf)

### Feature Engineering
1. Large language models for automated data science: Introducing caafe for context-aware automated feature engineering [[paper]](https://arxiv.org/pdf/2305.03403)
2. Large language models for constructing and optimizing machine learning workflows: A survey [[paper]](https://arxiv.org/pdf/2411.10478)
3. Dynamic and adaptive feature generation with llm [[paper]](https://arxiv.org/pdf/2406.03505)
4. LLM-based feature generation from text for interpretable machine learning [[paper]](https://arxiv.org/pdf/2409.07132)
5. Informing reinforcement learning agents by grounding language to markov decision processes [[paper]](https://openreview.net/pdf?id=P4op21eju0)
6. Unsupervised extraction of dialogue policies from conversations [[paper]](https://aclanthology.org/2024.emnlp-main.1060.pdf)
7. Can chatgpt’s performance be improved on verb metaphor detection tasks? Bootstrapping and combining tacit knowledge  [[paper]](https://aclanthology.org/2024.acl-long.57.pdf)


---

## Model-Centric LLM-in-the-loop

### Active Learning With LLM 

1. LLMaAA: Making Large Language Models as Active Annotators [[paper]](https://aclanthology.org/2023.findings-emnlp.872.pdf)
2. Enhancing text classification through llm-driven active learning and human
annotation [[paper]](https://aclanthology.org/2024.law-1.10.pdf)
3. LLMaAA: Making large language models as active annotators [[paper]](https://aclanthology.org/2023.findings-emnlp.872.pdf)
4. Enhancing text classification through llm-driven active learning and human
annotation [[paper]](https://aclanthology.org/2024.law-1.10.pdf)
5. A survey of confidence estimation and calibration in large language models [[paper]](https://aclanthology.org/2024.naacl-long.366.pdf)
6. Can LLMs express their uncertainty? an empirical evaluation of confidence elicitation in LLMs [[paper]](https://arxiv.org/pdf/2306.13063)
7.  Generalized category discovery with large language models in the loop [[paper]](https://aclanthology.org/2024.findings-acl.512.pdf)
8.  Dial-in llm: Human-aligned dialogue intent clustering with llm-in-the-loop [[paper]](https://arxiv.org/pdf/2412.09049)
9.  Neural topic modeling with large language models in the loop [[paper]](https://arxiv.org/pdf/2411.08534)


### Reinforcement Learning With LLM
1. Survey on large language model-enhanced reinforcement learning: Concept, taxonomy, and methods [[paper]](https://arxiv.org/pdf/2404.00282)
2. Aligning large language models with human preferences through representation engineering [[paper]](https://aclanthology.org/2024.acl-long.572.pdf)
3.  A survey on enhancing reinforcement learning in complex environments: Insights from human and llm feedback [[paper]](https://arxiv.org/pdf/2411.13410)
4.  Guiding pretraining in reinforcement learning with large language models [[paper]](https://arxiv.org/pdf/2302.06692)
5.  Reward design with language models [[paper]](https://arxiv.org/pdf/2303.00001)
6.  Reinforcement learning from llm feedback to counteract goal misgeneralization [[paper]](https://arxiv.org/pdf/2401.07181)
7.  Lagr-seq: Language-guided reinforcement learning with sample-efficient querying [[paper]](https://arxiv.org/pdf/2308.13542)
8.  LLM augmented hierarchical agents [[paper]](https://arxiv.org/pdf/2311.05596)
9.  Experience sharing and human-in-the-loop optimization for federated robot navigation recommendation [[paper]](https://link.springer.com/chapter/10.1007/978-3-031-51026-7_16)
10.   Federated machine learning: Concept and applications [[paper]](https://arxiv.org/pdf/1902.04885)


---

## Task-Centric LLM-in-the-loop
1. Harnessing large language models as post-hoc correctors [[paper]](https://aclanthology.org/2024.findings-acl.867.pdf)
2. Hyporadise: An open baseline for generative speech recognition with large language models [[paper]](https://arxiv.org/pdf/2309.15701)
3. Gentranslate: Large language models are generative multilingual speech and machine translators [[paper]](https://arxiv.org/pdf/2402.06894)
4. Large language models enable few-shot clustering [[paper]](https://aclanthology.org/2024.tacl-1.18.pdf)
5. Dial-in llm: Human-aligned dialogue intent clustering with llm-in-the-loop [[paper]](https://arxiv.org/pdf/2412.09049)
6. Enhanced short text modeling: Leveraging large language models for topic refinement [[paper]](https://arxiv.org/pdf/2403.17706)
7. Generating descriptive explanations of machine learning models using llm [[paper]](https://www.computer.org/csdl/proceedings-article/bigdata/2024/10825667/23ykhJUQhpK)
8. Improving hierarchical text clustering with llm-guided multi-view cluster representation [[paper]](https://aclanthology.org/2024.emnlp-industry.54.pdf)
9. Generalized category discovery with large language models in the loop [[paper]](https://aclanthology.org/2024.findings-acl.512.pdf)
10. Uncovering latent arguments in social media messaging by employing llms-in-the-loop strategy [[paper]](https://arxiv.org/pdf/2404.10259)
11. G-eval: Nlg evaluation using gpt-4 with better human alignment [[paper]](https://aclanthology.org/2023.emnlp-main.153.pdf)

---

## Crowdsourcing
1. Spatial crowdsourcing: a survey [[paper]](https://link.springer.com/article/10.1007/s00778-019-00568-7)
2. ChatGPT outperforms crowd workers for text-annotation tasks [[paper]](https://www.pnas.org/doi/abs/10.1073/pnas.2305016120)
3. Investigating and mitigating biases in crowdsourced data [[paper]](https://dl.acm.org/doi/10.1145/3462204.3481729)
4. AnnoLLM: Making Large Language Models to Be Better Crowdsourced Annotators [[paper]](https://aclanthology.org/2024.naacl-industry.15/)
5. Wisdom of the silicon crowd: LLM ensemble prediction capabilities rival human crowd accuracy [[paper]](https://www.science.org/doi/10.1126/sciadv.adp1528)
6. LMTurk: Few-Shot Learners as Crowdsourcing Workers in a Language-Model-as-a-Service Framework [[paper]](https://aclanthology.org/2022.findings-naacl.51/)
7. LLMs to Replace Crowdsourcing For Parallel Data Creation? The Case of Text Detoxification [[paper]](https://aclanthology.org/2024.findings-emnlp.839/)
8. ChatGPT-4 Outperforms Experts and Crowd Workers in Annotating Political Twitter Messages with Zero-Shot Learning [[paper]](https://arxiv.org/abs/2304.06588)
9. ChatGPT to Replace Crowdsourcing of Paraphrases for Intent Classification: Higher Diversity and Comparable Model Robustness [[paper]](https://aclanthology.org/2023.emnlp-main.117/)
10. A Comparative Study on Annotation Quality of Crowdsourcing and LLm Via Label Aggregation [[paper]](https://ieeexplore.ieee.org/document/10447803)

---

## Summary


| **Venue** | **Year: Count**                                      | **Total** |
|---------------|-------------------------------------------------------|-----------|
| ArXiv         | {2023: 10, 2024: 20}                                 | 30        |
| EMNLP         | {2016: 1, 2023: 7, 2024: 5}                          | 13        |
| ACL           | {2019: 1, 2020: 2, 2024: 6}                          | 9         |
| ICLR          | {2015: 1, 2022: 1, 2023: 2, 2024: 3}                 | 7         |
| NAACL         | {2022: 1, 2024: 6}                                   | 7         |
| NeurIPS       | {2014: 1, 2020: 2, 2021: 1, 2024: 2}                 | 6         |
| ICML          | {2016: 1, 2022: 1, 2023: 1, 2024: 1}                 | 4         |
| AAAI          | {2021: 1, 2023: 1, 2024: 1, 2025: 1}                 | 4         |
| VLDB          | {2013: 1, 2014: 1, 2019: 1}                          | 3         |
| TACL          | {2020: 1, 2024: 1}                                   | 2         |
| Others        | {2013: 1, 2016: 1, 2017: 2, 2019: 3, 2021: 2, 2022: 3, 2023: 5, 2024: 6} | 24        |
| **Total**     |                                                       | **109**   |


